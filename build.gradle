apply plugin: 'java'
apply plugin: 'scala'

version = '1.0'

repositories {
    mavenCentral()
    jcenter()
    mavenLocal()
}

def propertyOrDefault(String propertyName, String propertyDefault) {
    project.hasProperty(propertyName) ? project.getProperty(propertyName) : propertyDefault
}

if (!hasProperty('against')) {
    ext.against = 'dse'    // default
}

ext {
    scalaVersion = propertyOrDefault("scalaVersion", '2.11.12')
    ScalaVersionShort = scalaVersion.tokenize(".").subList(0,2).join(".")
    SparkVersion = propertyOrDefault("sparkVersion", '2.0.2')
    SparkConnectorVersion = propertyOrDefault("sparkConnectorVersion", '2.0.6')
    Home = System.env.HOME
    DseHome = System.env.DSE_HOME ?: "$Home/dse"
    DseResources = System.env.DSE_RESOURCES ?: "$Home/dse/resources/"
    mainClassName = "com.datastax.sparkstress.SparkCassandraStress"
    SparkCCHome =  System.env.SPARKCC_HOME ?: "$Home/repos/spark-cassandra-connector/"
}

def determineConnectorVersion() {
    if (against == 'dse') {
        def connector = fileTree(dir: "$DseHome", include: '**/spark*connector*jar')
        def connectorJarName = (connector as List)[0].name
        def match = connectorJarName =~ /.*-(\d+\.\d+.\d+).*jar/
        assert match.find(), "Unable to find Spark Cassandra Connector"
        assert match.group(1).length() != 0, "Unable to determine version from " + match.group(0)
        println("Connector Version = " + match.group(1))
        return match.group(1)
    }
    if (against == 'maven') {
        return System.env.CONNECTOR_VERSION ?: SparkConnectorVersion
    }
}

def ConnectorVersion = determineConnectorVersion()


def deps = [
        dse   : {
            println "Using DSE libraries"
            [
                    'dse/lib',
                    'driver/lib',
                    'cassandra/lib',
                    'spark/lib',
                    'shark/lib',
                    'hadoop',
                    'hadoop/lib',
                    'hadoop2-client',
                    'hadoop2-client/lib',
                    'lib',
                    'common',
                    ''
            ].each { dir ->

                provided fileTree(dir: "$DseResources/$dir", include: '*.jar')
            }

            [
                    '/build',
                    '/lib',
                    '/build/lib'
            ].each { dir ->
                provided fileTree(dir: "$DseHome/$dir", include: '*.jar')
            }


        },
        maven : {
            println "Using Maven Libraries"
            compile "com.datastax.spark:spark-cassandra-connector_$ScalaVersionShort:$ConnectorVersion"
            provided "org.apache.spark:spark-core_$ScalaVersionShort:$SparkVersion"
            provided "org.apache.spark:spark-streaming_$ScalaVersionShort:$SparkVersion"
            provided "org.apache.spark:spark-sql_$ScalaVersionShort:$SparkVersion"
        },
        source: {
            println "Using Assembly Jar from Source Repo"

            compile fileTree(dir: "$SparkCCHome/spark-cassandra-connector/target/scala-$ScalaVersionShort/", include: "*.jar")
            provided "org.apache.spark:spark-core_$ScalaVersionShort:$SparkVersion"
            provided "org.apache.spark:spark-streaming_$ScalaVersionShort:$SparkVersion"
            provided "org.apache.spark:spark-sql_$ScalaVersionShort:$SparkVersion"
        },

]

task build_connector(type: Exec) {
    workingDir SparkCCHome
    commandLine 'sbt/sbt', 'clean'
    commandLine 'sbt/sbt', 'assembly'
}

if (against == 'source') {
    jar.dependsOn build_connector
}


jar {
    manifest.attributes("Main-Class": mainClassName)
    baseName = "SparkCassandraStress"
    from {
        (configurations.runtime - configurations.provided).collect {
            it.isDirectory() ? it : zipTree(it)
        }
    } {
        exclude "META-INF/*.SF"
        exclude "META-INF/*.DSA"
        exclude "META-INF/*.RSA"
    }
}

configurations {
    provided
    compile.extendsFrom provided
}


repositories {
    mavenCentral()
}

test {
    if(against == 'dse') {
        exclude '**/NonDseWriteTaskTests/**'
    }
    testLogging {
        events "passed", "skipped", "failed", "standardOut", "standardError"
    }
}

dependencies {
    testCompile group: 'junit', name: 'junit', version: '4.12'
    testCompile "org.scala-lang:scala-library:$scalaVersion"
    testCompile "org.scalatest:scalatest_$ScalaVersionShort:2.2.4"
    compile("org.reflections:reflections:0.9.9") {
        exclude group: 'dom4j', module: 'dom4j'
        exclude group: 'org.javassist', module: 'javassist'
    }
    compile "com.github.scopt:scopt_$ScalaVersionShort:3.7.0"

    println "Checking dependency flag: $against"

}

dependencies deps[(against)]

sourceSets {
    main {
        scala {
            srcDirs = ['src/main/scala', 'src/main/java']
            if (against == 'dse') {
                srcDirs += 'src/dse'
            } else {
                srcDirs += 'src/apache'
            }

            //Api Change Catcher -- This is done to catch the CassandraCount Change in connector 1.2.4
            println("Connector version -> " + ConnectorVersion)
            def (major, minor, patch) = ConnectorVersion.split(/\./,3).collect { (it.find(/^\d+/).toInteger()) }
            if ((against == 'dse' && major <= 6 && minor <= 8) || (major <= 2 && minor <= 4)){
                println("using spark-cassandra-stress compatibility layer for DSE 6.8.x (or older) and SCC 2.4.x (or older)")
                srcDirs += 'src/connector/x-6.8'
            } else {
                println("using spark-cassandra-stress compatibility layer for DSE 6.9.x (or newer) and SCC 2.5.x (or newer)")
                srcDirs += 'src/connector/default'
            }

        }
    }
}
