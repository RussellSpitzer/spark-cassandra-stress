apply plugin: 'java'
apply plugin: 'scala'

version = '1.1'

if (!hasProperty('against')) {
    ext.against = 'maven'    // default
}

ext.Home = System.env.HOME
ext.DseHome = System.env.DSE_HOME ?: "$Home/dse"
ext.DseResources = System.env.DSE_RESOURCES ?: "$Home/dse/resources/"
ext.mainClassName = "com.datastax.sparkstress.SparkCassandraStress"

def sparkLibDir = System.getenv("SPARK_LIB_DIR") ?: ext.DseHome+"/resources/spark/lib"
def dseScalaCompilerJar = fileTree(dir:"$sparkLibDir").matching{include "scala-compiler*jar"}
def dseScalaVersionArr = dseScalaCompilerJar.getSingleFile().toString().split("-").last().split(".jar").first().split("\\.")
ext.scalaVersion = String.join(".", Arrays.copyOfRange(dseScalaVersionArr,0,2))
println("Compiling with Scala version "+ext.scalaVersion)

ext.defaultConnector = "1.6.3"
ext.defaultSpark = "1.6.2"

def determineConnectorVersion() {
    if (against == 'dse') {
        def connector = fileTree(dir: "$DseResources/spark/lib", include: 'spark*connector*_*.jar')
        def connectorJarName = (connector as List)[0].name
        def match = connectorJarName =~ /connector.*_.*-(\d+\.\d+.\d+).*\.jar/
        assert match.find(), "Unable to find Spark Cassandra Connector"
        assert match.group(1).length() != 0, "Unable to determine version from " + match.group(0)
        println("Connector Version = " + match.group(1))
        return match.group(1)
    }
    if (against == 'maven') {
        return System.env.CONNECTOR_VERSION ?: defaultConnector
    }
}

// Parameters for buliding against Maven Libs
def ConnectorVersion = determineConnectorVersion()
def SparkVersion = System.env.SPARK_VERSION ?: defaultSpark

// Parameter for building against Connector Repository
def SparkCCHome = System.env.SPARKCC_HOME ?:
        "$Home/repos/spark-cassandra-connector/"

def deps = [
        dse   : {
            println "Using DSE libraries"
            [
                    'dse/lib',
                    'driver/lib',
                    'cassandra/lib',
                    'spark/lib',
                    'shark/lib',
                    'hadoop',
                    'hadoop/lib',
                    'hadoop2-client',
                    'hadoop2-client/lib',
                    'lib',
                    'common',
                    ''
            ].each { dir ->

                provided fileTree(dir: "$DseResources/$dir", include: '*.jar')
            }

            [
                    '/build',
                    '/lib',
                    '/build/lib'
            ].each { dir ->
                provided fileTree(dir: "$DseHome/$dir", include: '*.jar')
            }


        },
        maven : {
            println "Using Maven Libraries"
            compile "com.datastax.spark:spark-cassandra-connector_$scalaVersion:$ConnectorVersion"
            provided "org.apache.spark:spark-core_$scalaVersion:$SparkVersion"
            provided "org.apache.spark:spark-sql_$scalaVersion:$SparkVersion"
            provided "org.apache.spark:spark-streaming_$scalaVersion:$SparkVersion"
        },
        source: {
            println "Using Assembly Jar from Source Repo"
            compile fileTree(dir: "$SparkCCHome/spark-cassandra-connector/target/scala-$scalaVersion/", include: "*.jar")
            provided "org.apache.spark:spark-sql_$scalaVersion:$SparkVersion"
            provided "org.apache.spark:spark-core_$scalaVersion:$SparkVersion"
            provided "org.apache.spark:spark-streaming_$scalaVersion:$SparkVersion"
        },

]

task build_connector(type: Exec) {
    workingDir SparkCCHome
    commandLine 'sbt/sbt', 'clean'
    commandLine 'sbt/sbt', 'assembly'
}

if (against == 'source') {
    jar.dependsOn build_connector
}


jar {
    manifest.attributes("Main-Class": mainClassName)
    baseName = "SparkCassandraStress"
    from {
        (configurations.runtime - configurations.provided).collect {
            it.isDirectory() ? it : zipTree(it)
        }
    } {
        exclude "META-INF/*.SF"
        exclude "META-INF/*.DSA"
        exclude "META-INF/*.RSA"
    }
}

configurations {
    provided
    compile.extendsFrom provided
}

repositories {
    mavenCentral()
}

test {
    if(against == 'dse') {
        exclude '**/NonDseWriteTaskTests/**'
    }
    testLogging {
        events "passed", "skipped", "failed", "standardOut", "standardError"
    }
}

dependencies {
    testCompile "com.datastax.spark:spark-cassandra-connector-embedded_$scalaVersion:$ConnectorVersion"
    testCompile "org.scalatest:scalatest_$scalaVersion:2.2.4"
    compile "com.github.scopt:scopt_$scalaVersion:3.2.0"
    compile "joda-time:joda-time:2.8.1"

    println "Checking dependency flag: $against"

}

dependencies deps[(against)]

sourceSets {
    main {
        scala {
            srcDirs = ['src/main/scala']
            if (against == 'dse') {
                srcDirs += 'src/dse'
            } else {
                srcDirs += 'src/apache'
            }

            //Api Change Catcher -- This is done to catch the CassandraCount Change in connector 1.2.4
            println(ConnectorVersion)
            def (major, minor, patch) = ConnectorVersion.split(/\./,3).collect { (it.find(/^\d+/).toInteger()) }
            if (major == 1 && minor == 2 && patch < 4){
                println("using special 1.2.0 -1.2.3 stubs")
                srcDirs += 'src/connector/1.2.0to1.2.3'
            } else if (major <= 1 && minor <= 1 ) {
                println("using pre-Connector 1.2.0 stubs")
                srcDirs += 'src/connector/lessThan1.2.0'
            } else { srcDirs += 'src/connector/default' }

        }
    }
}
